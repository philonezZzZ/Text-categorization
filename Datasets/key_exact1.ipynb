{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Philone\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting key-word of each article of X\n",
    "def key_extract(X,Y,weights,spilt_factor=0.1,print_detail = 0):  \n",
    "    ret_X = []                         #save the key-word of all article \n",
    "    for i in range(len(X)):\n",
    "        words = X[i]                  #each word in article-i \n",
    "        temp_X = []                     #save the key-word repeated of an article \n",
    "        for word in words:\n",
    "            temp = weights[int(word)][Y[i]]\n",
    "            if temp > spilt_factor:\n",
    "                temp_X.append(word)     #save the word\n",
    "        if i<print_detail:\n",
    "            print('Line'+str(i)+': total word: '+str(len(words))+' key word(repition): '+str(len(temp_X))+' percentage: '+str(len(temp_X)*1./len(words)))\n",
    "            print(temp_X)\n",
    "        ret_X.append(temp_X)             #save the article\n",
    "    return ret_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(X):\n",
    "    words = set()\n",
    "    for x in X:\n",
    "        for word in x:\n",
    "            words.add(word)\n",
    "    return len(words),words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_count(X,key_word,print_detail = 0):\n",
    "    X_key_word = []   # key-word repeated of all articles\n",
    "    no_word_article = []   #sum of article without key-word \n",
    "    for i in range(len(X)):\n",
    "        x_key_word = []    #key-word repeated of an article\n",
    "        dif_word = set()\n",
    "        for word in X[i]:\n",
    "            if word in key_word:\n",
    "                x_key_word.append(word)\n",
    "                dif_word.add(word)\n",
    "        if len(dif_word) == 0:\n",
    "            no_word_article.append(i)\n",
    "        X_key_word.append(x_key_word)\n",
    "        if i<print_detail:\n",
    "            print('line '+str(i)+' existing word: '+str(len(x_key_word))+' '+str(len(x_key_word)*1./len(X[i]))+' different word: '+str(len(dif_word))+' '+str(len(dif_word)*1./len(X[i])))\n",
    "    return no_word_article,X_key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_extract_compliment(X,key_word):\n",
    "    X_key_word = []   # key-word repeated of all articles\n",
    "    for i in range(len(X)):\n",
    "        x_key_word = []    #key-word repeated of an article\n",
    "        no_word = True\n",
    "        for word in X[i]:\n",
    "            if word in key_word:\n",
    "                x_key_word.append(word)\n",
    "                no_word = False\n",
    "        if no_word:\n",
    "            for j in range(100):\n",
    "                k = random.randint(0,len(X[i])-1)\n",
    "                x_key_word.append(X[i][k])\n",
    "        X_key_word.append(x_key_word)\n",
    "    return X_key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_weights(X,Y,weights1,weights1_split_factor,weights2,weights2_split_factor):\n",
    "    log = {'num_of_key_word_after_weights1':0,'num_of_key_word_after_weights2':0,'no_word_article':0}\n",
    "    \n",
    "    weights_X = key_extract(X,Y,weights1,weights1_split_factor)    #using weights1 to get key-word\n",
    "    log['num_of_key_word_after_weights1'],_ = word_count(weights_X)\n",
    "    \n",
    "    weights_X = key_extract(weights_X,Y,weights2,weights2_split_factor)  #using weights2 to get key-word\n",
    "    log['num_of_key_word_after_weights2'],key_word_set = word_count(weights_X)\n",
    "    \n",
    "    log['no_word_article'],_ = article_count(X,key_word_set,print_detail = 0)  #counting the missing articles\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator version:\n",
    "def choose_weights_generator(X,Y,weights1,weights1_split_factor,weights2):\n",
    "    log = {'num_of_key_word_after_weights1':0,'num_of_key_word_after_weights2':0,'no_word_article':0}\n",
    "    \n",
    "    weights1_X = key_extract(X,Y,weights1,weights1_split_factor)    #using weights1 to get key-word\n",
    "    log['num_of_key_word_after_weights1'],_ = word_count(weights1_X)\n",
    "    for j in range(11):\n",
    "        log['num_of_key_word_after_weights2'] = 0\n",
    "        log['no_word_article'] = 0\n",
    "        weights2_X = key_extract(weights1_X,Y,weights2,j/10.)  #using weights2 to get key-word\n",
    "        log['num_of_key_word_after_weights2'],key_word_set = word_count(weights2_X)\n",
    "    \n",
    "        log['no_word_article'],_ = article_count(X,key_word_set,print_detail = 0)  #counting the missing articles\n",
    "        yield log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./../data/train_set.csv')\n",
    "weights1 = np.load('./../data/word_weigt.npy')\n",
    "weights2 = np.load('./../data/word_article.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X = train_set['word_seg'].values\n",
    "Y = train_set['class'].values\n",
    "X = []\n",
    "for x in pre_X:\n",
    "    X.append(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102277\n",
      "817383\n"
     ]
    }
   ],
   "source": [
    "ret_orig_X = key_extract(X,Y,weights1,0.3)\n",
    "print(len(ret_orig_X))\n",
    "word_num,_ = word_count(ret_orig_X)\n",
    "print(word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102277\n",
      "2742\n"
     ]
    }
   ],
   "source": [
    "ret_orig_X = key_extract(ret_orig_X,Y,weights2,0.1)\n",
    "print(len(ret_orig_X))\n",
    "word_num,key_word = word_count(ret_orig_X)\n",
    "print(word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = article_extract_compliment(X,key_word)\n",
    "f=open('./../data/train_X.pkl','wb')  \n",
    "pickle.dump(train_X,f,0)  \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1246\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in train_X:\n",
    "    if(len(i) == 100):\n",
    "        j+=1\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(no_word_article))\n",
    "\n",
    "missing_Y = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for each in no_word_article:\n",
    "    missing_Y[Y[each]] +=1\n",
    "    \n",
    "print(missing_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(choose_weights(X,Y,weights1,0.4,weights2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "     for j in range(11):\n",
    "        log = choose_weights_generator(X,Y,weights1,i/10.,weights2,j/10.)\n",
    "        print('('+str(i/10.)+','+str(j/10.)+'):',end = '')\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    j = 0\n",
    "    for log in choose_weights_generator(X,Y,weights1,i/10.,weights2):\n",
    "        print('('+str(i/10.)+','+str(j/10.)+'):',end = '')\n",
    "        print(log)\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
